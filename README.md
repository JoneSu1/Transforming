# Transforming

**UC Berkeley的博士生Eric Tzeng发表在ICCV 2015上的文章《Simultaneous Deep Transfer Across Domains and Tasks》**

Motivation
作者提到了两种层次的transfer：

**domain transfer**：就是适配分布，特别地是指适配marginal distribution，但是没有考虑类别信息。如何做domain transfer：在传统深度网路的loss上，再加另一个confusion loss，作为classifier能否将两个domain进行分开的loss。两个loss一起计算，就是domain transfer。
**task transfer**：就是利用class之间的相似度，其实特指的是conditional distribution。类别之间有相似度，要利用上。类别之间的相似度：比如一个杯子与瓶子更相似，而与键盘不相似。文章的原话：it does not necessarily align the classes in the target with those in the source. Thus, we also explicity transfer the similarity structure amongst categories.

迁移学习其实是基于深度学习的原理，在进行训练的时候，前几层网络是会学习一些普遍存在的特征（如果是图像数据作为input，那前几层就是在学习边缘特征，比如哪些轮廓是属于物品，哪些是树木）。 所以我们可以设置一个预训练的模型（数据是基于EP和其他的promoter数据，数据量够大同时）

**创建预训练模型**
Dataset：我将使用5000个的EP数据和20000个的其他promoter数据作为输入，同时将在保证数据不拟合的情况下尽可能的加深layers。
而为了解决这个数据分布的不平均问题，我将通过重新设置数据权重的方式来解决.

### 如何调整权重？

在大多数机器学习框架中，你可以在训练过程中为每个类别设置不同的权重。权重通常根据类别中的样本数量的倒数来设置，或者可以根据实验结果来手动调整以达到最佳的分类效果。权重的基本思想是，数量较少的类别对应的每个样本在损失函数中占有更大的比重，从而使模型更加关注这些较少见的类别。

### 实际操作

例如，如果使用二元交叉熵作为损失函数，你可以这样设置权重：

- 对于EP类别（正类），权重可以设置为：

权重_EP = 总样本数 / (EP样本数 * 2)


- 对于其他类别（负类），权重可以设置为：

权重_其他 = 总样本数 / (其他样本数 * 2)

这样，模型在训练过程中会更加注重较少的EP样本，尽管它们在数据集中的数量较少。

**在进行正式训练时候**

- **在迁移学习和微调阶段的考虑**

重新评估权重：在迁移学习或微调模型时，你可能需要重新评估和调整类别权重，特别是如果目标任务（比如EP与control的分类）的数据分布与预训练任务不同。权重应该根据新任务的数据分布进行调整。

继续学习的灵活性：预训练模型已经学习到了一些通用的特征或针对特定少数类别的识别能力，但在微调阶段，模型将继续根据新的数据分布和任务目标进行调整。这意味着即使预训练阶段使用了特定的权重策略，微调阶段仍有机会根据新任务的需求优化模型性能。

权重和微调策略的结合使用：预训练模型在进行微调时，除了调整权重外，还可以采用其他策略，如冻结部分网络层、调整学习率、使用不同的优化器等，以适应新任务的具体需求。

总之，预训练阶段调整权重是为了解决特定的数据不平衡问题，而在后续的迁移学习或微调阶段，你有机会进一步调整模型，以最大化在新任务上的性能。预训练阶段所做的权重调整是一个有益的起点，它不会限制你在后续阶段对模型进行有效训练和调整的能力。
